{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average the results from lasso, ridge and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train_clean.csv\")\n",
    "test = pd.read_csv(\"../data/test_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the 'Id' column\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "#Now drop the 'Id' colum since it's unnecessary for the prediction process.\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)\n",
    "\n",
    "ytrain = train[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine data\n",
    "train.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
    "test.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "train.drop(['SalePrice'], axis=1, inplace=True)\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSSubClass should be string\n",
    "all_data[\"MSSubClass\"] = all_data[\"MSSubClass\"].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all continuous variables\n",
    "all_non_object = all_data.dtypes[all_data.dtypes != \"object\"].index.tolist()\n",
    "# do not consider Year,Month and Qual as continuous\n",
    "year_month = [\"YearBuilt\", \"YearRemodAdd\",\"GarageYrBlt\",\"MoSold\",\"YrSold\",\n",
    "              \"OverallQual\",\"OverallCond\"]\n",
    "# numeric_features\n",
    "numeric_features = list(set(all_non_object)-set(year_month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19 skewed numerical features to Box Cox transform\n"
     ]
    }
   ],
   "source": [
    "# Check the skew of all numerical features\n",
    "skewed_feats = all_data[numeric_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "\n",
    "# check skewness of numerical variables\n",
    "skewness = skewness[abs(skewness.Skew)>0.75]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    #all_data[feat] += 1\n",
    "    all_data[feat] = boxcox1p(all_data[feat], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_dict = {\"None\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "all_data[\"ExterQual\"] = all_data[\"ExterQual\"].map(qual_dict).astype(int)\n",
    "all_data[\"ExterCond\"] = all_data[\"ExterCond\"].map(qual_dict).astype(int)\n",
    "all_data[\"BsmtQual\"] = all_data[\"BsmtQual\"].map(qual_dict).astype(int)\n",
    "all_data[\"BsmtCond\"] = all_data[\"BsmtCond\"].map(qual_dict).astype(int)\n",
    "all_data[\"HeatingQC\"] = all_data[\"HeatingQC\"].map(qual_dict).astype(int)\n",
    "all_data[\"KitchenQual\"] = all_data[\"KitchenQual\"].map(qual_dict).astype(int)\n",
    "all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].map(qual_dict).astype(int)\n",
    "all_data[\"GarageQual\"] = all_data[\"GarageQual\"].map(qual_dict).astype(int)\n",
    "all_data[\"GarageCond\"] = all_data[\"GarageCond\"].map(qual_dict).astype(int)\n",
    "\n",
    "all_data[\"BsmtExposure\"] = all_data[\"BsmtExposure\"].map(\n",
    "        {\"None\": 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4}).astype(int)\n",
    "\n",
    "bsmt_fin_dict = {\"None\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n",
    "all_data[\"BsmtFinType1\"] = all_data[\"BsmtFinType1\"].map(bsmt_fin_dict).astype(int)\n",
    "all_data[\"BsmtFinType2\"] = all_data[\"BsmtFinType2\"].map(bsmt_fin_dict).astype(int)\n",
    "\n",
    "all_data[\"Functional\"] = all_data[\"Functional\"].map(\n",
    "        {\"None\": 0, \"Sal\": 1, \"Sev\": 2, \"Maj2\": 3, \"Maj1\": 4, \n",
    "         \"Mod\": 5, \"Min2\": 6, \"Min1\": 7, \"Typ\": 8}).astype(int)\n",
    "\n",
    "all_data[\"GarageFinish\"] = all_data[\"GarageFinish\"].map(\n",
    "        {\"None\": 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3}).astype(int)\n",
    "\n",
    "all_data[\"Fence\"] = all_data[\"Fence\"].map(\n",
    "        {\"None\": 0, \"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4}).astype(int)\n",
    "\n",
    "all_data[\"PoolQC\"] = all_data[\"PoolQC\"].map(qual_dict).astype(int)\n",
    "\n",
    "# Most land slopes are gentle; treat the others as \"not gentle\".\n",
    "all_data[\"LandSlope\"] = (all_data[\"LandSlope\"] == \"Gtl\") * 1\n",
    "# IR2 and IR3 don't appear that often, so just make a distinction\n",
    "# between regular and irregular.\n",
    "all_data[\"LotShape\"] = (all_data[\"LotShape\"] == \"Reg\") * 1\n",
    "# Most properties use standard circuit breakers.\n",
    "all_data[\"Electrical\"] = (all_data[\"Electrical\"] == \"SBrkr\") * 1\n",
    "# Most have a paved drive. Treat dirt/gravel and partial pavement\n",
    "# as \"not paved\".\n",
    "all_data[\"PavedDrive\"] = (all_data[\"PavedDrive\"] == \"Y\") * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # label encoding\n",
    "# for c in all_data.columns:\n",
    "#     if all_data[c].dtype == 'object' or c in year_month:\n",
    "#         lbl = preprocessing.LabelEncoder()\n",
    "#         lbl.fit(list(all_data[c].values)) \n",
    "#         all_data[c] = lbl.transform(list(all_data[c].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2917, 233)\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.get_dummies(all_data)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:] #prediction data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, \n",
    "                                                    ytrain,\n",
    "                                                    train_size=1-test_size, \n",
    "                                                    test_size=test_size, \n",
    "                                                random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base models\n",
    "\n",
    "### XGBoost\n",
    "#### XGBoost parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma,\n",
    "                 alpha,lambd):\n",
    "\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['cosample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    params['alpha'] = max(alpha, 0)\n",
    "    params['lambda'] = max(lambd, 0)\n",
    "\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain, num_boost_round=num_rounds, nfold=5,\n",
    "             seed=random_state,\n",
    "             callbacks=[xgb.callback.early_stop(50)])\n",
    "\n",
    "    return -cv_result['test-rmse-mean'].values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data into d-matrices\n",
    "xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "xgtest = xgb.DMatrix(X_test, label=y_test)\n",
    "xgpred = xgb.DMatrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |     alpha |   colsample_bytree |     gamma |     lambd |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[184]\ttrain-rmse:0.144988+0.00294021\ttest-rmse:0.160464+0.0236221\n",
      "\n",
      "    1 | 00m08s | \u001b[35m  -0.16046\u001b[0m | \u001b[32m   1.8862\u001b[0m | \u001b[32m            0.9723\u001b[0m | \u001b[32m   0.8843\u001b[0m | \u001b[32m   1.0377\u001b[0m | \u001b[32m     8.8073\u001b[0m | \u001b[32m            8.5348\u001b[0m | \u001b[32m     0.9537\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[290]\ttrain-rmse:0.135499+0.00241771\ttest-rmse:0.15077+0.0204325\n",
      "\n",
      "    2 | 00m07s | \u001b[35m  -0.15077\u001b[0m | \u001b[32m   0.0206\u001b[0m | \u001b[32m            0.4115\u001b[0m | \u001b[32m   0.8403\u001b[0m | \u001b[32m   0.9575\u001b[0m | \u001b[32m     6.8756\u001b[0m | \u001b[32m            7.2194\u001b[0m | \u001b[32m     0.5135\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[418]\ttrain-rmse:0.133493+0.00323649\ttest-rmse:0.152374+0.0207302\n",
      "\n",
      "    3 | 00m18s |   -0.15237 |    0.4019 |             0.6868 |    0.7483 |    1.9950 |     11.5609 |             9.6296 |      0.8700 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[504]\ttrain-rmse:0.152237+0.00225644\ttest-rmse:0.166153+0.0213006\n",
      "\n",
      "    4 | 00m18s |   -0.16615 |    1.5316 |             0.5508 |    1.1202 |    0.2048 |     11.5720 |             5.9030 |      0.6122 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[163]\ttrain-rmse:0.134098+0.00317432\ttest-rmse:0.154702+0.0209825\n",
      "\n",
      "    5 | 00m08s |   -0.15470 |    0.4799 |             0.3359 |    0.7364 |    0.5034 |      8.7369 |             2.2603 |      0.9846 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |     alpha |   colsample_bytree |     gamma |     lambd |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[414]\ttrain-rmse:0.161797+0.00378944\ttest-rmse:0.174984+0.0200595\n",
      "\n",
      "    6 | 00m23s |   -0.17498 |    0.1089 |             0.8018 |    1.9965 |    1.9539 |      3.5143 |             1.3124 |      0.7857 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[377]\ttrain-rmse:0.0967002+0.00325496\ttest-rmse:0.127569+0.0159297\n",
      "\n",
      "    7 | 00m16s | \u001b[35m  -0.12757\u001b[0m | \u001b[32m   0.1696\u001b[0m | \u001b[32m            0.2817\u001b[0m | \u001b[32m   0.0925\u001b[0m | \u001b[32m   0.0162\u001b[0m | \u001b[32m     2.2994\u001b[0m | \u001b[32m            9.4084\u001b[0m | \u001b[32m     0.9313\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[505]\ttrain-rmse:0.0782156+0.00356644\ttest-rmse:0.122705+0.0180543\n",
      "\n",
      "    8 | 00m32s | \u001b[35m  -0.12271\u001b[0m | \u001b[32m   0.8695\u001b[0m | \u001b[32m            0.1000\u001b[0m | \u001b[32m   0.0000\u001b[0m | \u001b[32m   1.9034\u001b[0m | \u001b[32m     2.4700\u001b[0m | \u001b[32m            9.1405\u001b[0m | \u001b[32m     0.6471\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[221]\ttrain-rmse:0.0745486+0.00367195\ttest-rmse:0.120991+0.0137958\n",
      "\n",
      "    9 | 00m27s | \u001b[35m  -0.12099\u001b[0m | \u001b[32m   0.0000\u001b[0m | \u001b[32m            0.1000\u001b[0m | \u001b[32m   0.0000\u001b[0m | \u001b[32m   0.7421\u001b[0m | \u001b[32m     2.0000\u001b[0m | \u001b[32m            1.0866\u001b[0m | \u001b[32m     0.5000\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[300]\ttrain-rmse:0.077372+0.0040276\ttest-rmse:0.119735+0.0151874\n",
      "\n",
      "   10 | 00m29s | \u001b[35m  -0.11974\u001b[0m | \u001b[32m   0.0000\u001b[0m | \u001b[32m            0.9467\u001b[0m | \u001b[32m   0.0000\u001b[0m | \u001b[32m   1.9248\u001b[0m | \u001b[32m     2.0000\u001b[0m | \u001b[32m            9.8725\u001b[0m | \u001b[32m     0.5000\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[288]\ttrain-rmse:0.0743984+0.00354865\ttest-rmse:0.118719+0.0164882\n",
      "\n",
      "   11 | 00m36s | \u001b[35m  -0.11872\u001b[0m | \u001b[32m   0.0000\u001b[0m | \u001b[32m            0.1000\u001b[0m | \u001b[32m   0.0000\u001b[0m | \u001b[32m   2.0000\u001b[0m | \u001b[32m     2.0000\u001b[0m | \u001b[32m            6.1293\u001b[0m | \u001b[32m     0.6198\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[314]\ttrain-rmse:0.0681224+0.00340526\ttest-rmse:0.11811+0.0137561\n",
      "\n",
      "   12 | 00m34s | \u001b[35m  -0.11811\u001b[0m | \u001b[32m   0.0000\u001b[0m | \u001b[32m            0.1000\u001b[0m | \u001b[32m   0.0000\u001b[0m | \u001b[32m   2.0000\u001b[0m | \u001b[32m     2.4872\u001b[0m | \u001b[32m            1.0000\u001b[0m | \u001b[32m     0.5000\u001b[0m | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-2.2489414e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 48, 'nit': 4, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[311]\ttrain-rmse:0.0767028+0.00351455\ttest-rmse:0.118518+0.0160332\n",
      "\n",
      "   13 | 00m34s |   -0.11852 |    0.2397 |             0.1000 |    0.0000 |    2.0000 |      2.0000 |             5.1742 |      0.5000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n",
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n",
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n",
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n",
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n",
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n",
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n",
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n",
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[311]\ttrain-rmse:0.0862326+0.00264444\ttest-rmse:0.123621+0.0164946\n",
      "\n",
      "   14 | 00m32s |   -0.12362 |    0.1076 |             0.3275 |    0.0577 |    1.9842 |      2.0418 |             1.5912 |      0.8701 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[389]\ttrain-rmse:0.0426506+0.000403274\ttest-rmse:0.12483+0.0136338\n",
      "\n",
      "   15 | 00m47s |   -0.12483 |    0.1268 |             0.4652 |    0.0145 |    1.7713 |     11.9014 |             1.3155 |      0.5356 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-3.31488991e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 53, 'nit': 6, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[371]\ttrain-rmse:0.0703016+0.00342546\ttest-rmse:0.11905+0.0125093\n",
      "\n",
      "   16 | 00m34s |   -0.11905 |    0.0956 |             0.7475 |    0.0115 |    1.7356 |      2.3746 |             6.8153 |      0.5157 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[300]\ttrain-rmse:0.074621+0.0028524\ttest-rmse:0.118992+0.0148225\n",
      "\n",
      "   17 | 00m31s |   -0.11899 |    0.0718 |             0.8846 |    0.0265 |    1.8061 |      2.1945 |             4.8041 |      0.5272 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[304]\ttrain-rmse:0.0700302+0.00302086\ttest-rmse:0.11772+0.0146846\n",
      "\n",
      "   18 | 00m32s | \u001b[35m  -0.11772\u001b[0m | \u001b[32m   0.0074\u001b[0m | \u001b[32m            0.1611\u001b[0m | \u001b[32m   0.0176\u001b[0m | \u001b[32m   1.8549\u001b[0m | \u001b[32m     2.6089\u001b[0m | \u001b[32m            2.6088\u001b[0m | \u001b[32m     0.5243\u001b[0m | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-1.42720353e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 48, 'nit': 4, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1175]\ttrain-rmse:0.0899236+0.00178118\ttest-rmse:0.137567+0.0171291\n",
      "\n",
      "   19 | 01m00s |   -0.13757 |    1.9360 |             0.5390 |    0.0288 |    1.8564 |      8.8035 |             1.9628 |      0.5110 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[210]\ttrain-rmse:0.058804+0.00062095\ttest-rmse:0.120038+0.0154993\n",
      "\n",
      "   20 | 00m38s |   -0.12004 |    0.2190 |             0.9470 |    0.0243 |    0.1052 |     10.4680 |             9.7027 |      0.5303 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[504]\ttrain-rmse:0.0682994+0.00116291\ttest-rmse:0.127588+0.0149521\n",
      "\n",
      "   21 | 00m42s |   -0.12759 |    1.8536 |             0.1237 |    0.0054 |    0.0733 |     11.9419 |             9.7393 |      0.7284 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[201]\ttrain-rmse:0.0525402+0.000620586\ttest-rmse:0.124497+0.0157526\n",
      "\n",
      "   22 | 00m37s |   -0.12450 |    0.6218 |             0.9694 |    0.0044 |    0.0143 |     11.7600 |             9.4382 |      0.5725 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[87]\ttrain-rmse:0.0479948+0.00218194\ttest-rmse:0.124918+0.0147537\n",
      "\n",
      "   23 | 00m35s |   -0.12492 |    0.0000 |             0.1000 |    0.0000 |    0.0000 |     10.5370 |            10.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[619]\ttrain-rmse:0.0618702+0.00115138\ttest-rmse:0.135607+0.0195272\n",
      "\n",
      "   24 | 00m48s |   -0.13561 |    2.0000 |             1.0000 |    0.0000 |    0.0000 |     11.6586 |             1.0000 |      0.5000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-0.00012801]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 44, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[556]\ttrain-rmse:0.106516+0.00223467\ttest-rmse:0.13499+0.0189078\n",
      "\n",
      "   25 | 00m50s |   -0.13499 |    2.0000 |             1.0000 |    0.0485 |    2.0000 |     12.0000 |            10.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[152]\ttrain-rmse:0.0444772+0.00165467\ttest-rmse:0.12271+0.0155399\n",
      "\n",
      "   26 | 00m39s |   -0.12271 |    0.0064 |             0.8739 |    0.0032 |    1.5279 |      7.1312 |             9.6030 |      0.5160 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[133]\ttrain-rmse:0.0612004+0.00293504\ttest-rmse:0.126049+0.013692\n",
      "\n",
      "   27 | 00m37s |   -0.12605 |    0.0000 |             1.0000 |    0.0000 |    0.0000 |      4.8000 |            10.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[761]\ttrain-rmse:0.0926374+0.00281805\ttest-rmse:0.126897+0.0186556\n",
      "\n",
      "   28 | 00m43s |   -0.12690 |    2.0000 |             1.0000 |    0.0000 |    0.0000 |      2.0000 |             9.7180 |      0.5000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-9.29500679e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 51, 'nit': 4, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[152]\ttrain-rmse:0.0383384+0.00128678\ttest-rmse:0.123665+0.0125537\n",
      "\n",
      "   29 | 00m38s |   -0.12366 |    0.0000 |             1.0000 |    0.0000 |    0.0000 |      4.9337 |             1.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[132]\ttrain-rmse:0.0347894+0.00155219\ttest-rmse:0.122797+0.0156056\n",
      "\n",
      "   30 | 00m37s |   -0.12280 |    0.0000 |             1.0000 |    0.0000 |    1.9580 |      9.9668 |             5.4795 |      0.5000 | \n"
     ]
    }
   ],
   "source": [
    "num_rounds = 3000\n",
    "random_state = 42\n",
    "num_iter = 25\n",
    "init_points = 5\n",
    "params = {\n",
    "        'eta': 0.1,\n",
    "        'silent': 1,\n",
    "        'eval_metric': 'rmse',\n",
    "        'verbose_eval': True,\n",
    "        'seed': random_state\n",
    "    }\n",
    "\n",
    "xgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (1, 10),\n",
    "                                                'colsample_bytree': (0.1, 1),\n",
    "                                                'max_depth': (2, 12),\n",
    "                                                'subsample': (0.5, 1),\n",
    "                                                'gamma': (0, 2),\n",
    "                                                'alpha': (0, 2),\n",
    "                                                'lambd':(0, 2)\n",
    "                                                })\n",
    "\n",
    "xgbBO.maximize(init_points=init_points, n_iter=num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_params = xgbBO.res[\"max\"][\"max_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_child_weight': 2.6088424342829697,\n",
       " 'colsample_bytree': 0.16110241153357344,\n",
       " 'max_depth': 2.6088722740012296,\n",
       " 'subsample': 0.5242784376438836,\n",
       " 'gamma': 0.017567433812438527,\n",
       " 'alpha': 0.007361616117448788,\n",
       " 'lambd': 1.8548925645780876}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_child_weight': 3.7316987233785577,\n",
       " 'colsample_bytree': 0.18788694151572233,\n",
       " 'max_depth': 2.58392470353414,\n",
       " 'subsample': 0.6833400971272943,\n",
       " 'gamma': 0.027285193517196715,\n",
       " 'alpha': 0.04647160701094655,\n",
       " 'lambd': 1.8796361500915535}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgb best params with train/test\n",
    "# {'min_child_weight': 3.7316987233785577,\n",
    "#  'colsample_bytree': 0.18788694151572233,\n",
    "#  'max_depth': 2.58392470353414,\n",
    "#  'subsample': 0.6833400971272943,\n",
    "#  'gamma': 0.027285193517196715,\n",
    "#  'alpha': 0.04647160701094655,\n",
    "#  'lambd': 1.8796361500915535}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':int(round(bayesian_params[\"max_depth\"])),\n",
    "    'min_child_weight': bayesian_params[\"min_child_weight\"],\n",
    "    'eta':.1,\n",
    "    'subsample': bayesian_params['subsample'],\n",
    "    'colsample_bytree': bayesian_params['colsample_bytree'],\n",
    "    'gamma':bayesian_params['gamma'],\n",
    "    'alpha':bayesian_params['alpha'],\n",
    "    'lambda':bayesian_params[\"lambd\"],\n",
    "    # Other parameters\n",
    "    'objective':'reg:linear',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = 3200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dont run\n",
    "# cv_results = xgb.cv(\n",
    "#     params,\n",
    "#     xgtrain,\n",
    "#     num_boost_round=num_boost_round,\n",
    "#     seed=42,\n",
    "#     nfold=5,\n",
    "#     metrics={'rmse'},\n",
    "#     early_stopping_rounds=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1173928"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv_results['test-rmse-mean'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter ETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.11 µs\n",
      "CV with eta=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:25: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\n",
      "will be corrected to return the positional minimum in the future.\n",
      "Use 'series.values.argmin' to get the position of the minimum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tRMSE 0.13783099999999998 for 121 rounds\n",
      "\n",
      "CV with eta=0.2\n",
      "\tRMSE 0.12996259999999998 for 113 rounds\n",
      "\n",
      "CV with eta=0.1\n",
      "\tRMSE 0.1215394 for 207 rounds\n",
      "\n",
      "CV with eta=0.05\n",
      "\tRMSE 0.11942060000000002 for 420 rounds\n",
      "\n",
      "CV with eta=0.01\n",
      "\tRMSE 0.11675060000000001 for 1715 rounds\n",
      "\n",
      "CV with eta=0.005\n",
      "\tRMSE 0.11678439999999998 for 3074 rounds\n",
      "\n",
      "Best params: 0.01, RMSE: 0.11675060000000001\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# This can take some time…\n",
    "min_rmse = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "\n",
    "    # We update our parameters\n",
    "    params['eta'] = eta\n",
    "\n",
    "    # Run and time CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        xgtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics=['rmse'],\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best score\n",
    "    mean_rmse = cv_results['test-rmse-mean'].min()\n",
    "    boost_rounds = cv_results['test-rmse-mean'].argmin()\n",
    "    print(\"\\tRMSE {} for {} rounds\\n\".format(mean_rmse, boost_rounds))\n",
    "    if mean_rmse < min_rmse:\n",
    "        min_rmse = mean_rmse\n",
    "        best_params = eta\n",
    "\n",
    "print(\"Best params: {}, RMSE: {}\".format(best_params, min_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['eta'] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3,\n",
       " 'min_child_weight': 2.6088424342829697,\n",
       " 'eta': 0.01,\n",
       " 'subsample': 0.5242784376438836,\n",
       " 'colsample_bytree': 0.16110241153357344,\n",
       " 'gamma': 0.017567433812438527,\n",
       " 'alpha': 0.007361616117448788,\n",
       " 'lambda': 1.8548925645780876,\n",
       " 'objective': 'reg:linear'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(colsample_bytree=params['colsample_bytree'], \n",
    "                             gamma=params['gamma'], \n",
    "                             learning_rate=params['eta'], max_depth=int(round(params['max_depth'])), \n",
    "                             min_child_weight=params['min_child_weight'], n_estimators=2200,\n",
    "                             reg_alpha=params['alpha'], reg_lambda=params['lambda'],\n",
    "                             subsample=params['subsample'], silent=1,\n",
    "                             random_state =42, nthread = -1)\n",
    "\n",
    "model_xgb.fit(X_train, y_train, early_stopping_rounds=10, \n",
    "             eval_set=[(X_test, y_test)], verbose=False)\n",
    "  \n",
    "y_train_xgb = model_xgb.predict(X_train)\n",
    "y_test_xgb = model_xgb.predict(X_test)\n",
    "xgb_prediction = model_xgb.predict(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO model parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lassocv = linear_model.LassoCV(cv=10, random_state=5, alphas=[0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n",
    "                          0.3, 0.6, 1, 3, 6, 10, 30, 60, 100])\n",
    "lassocv.fit(X_train, y_train)\n",
    "\n",
    "y_train_las = lassocv.predict(X_train)\n",
    "y_test_las = lassocv.predict(X_test)\n",
    "las_prediction = lassocv.predict(test)\n",
    "\n",
    "# lassocv_score = lassocv.score(train, ytrain)\n",
    "lassocv_alpha = lassocv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassocv_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = linear_model.Lasso(alpha=lassocv_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge\n",
    "### Ridge model parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv = linear_model.RidgeCV(cv=5, alphas=[0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n",
    "                          0.3, 0.6, 1, 3, 6, 10, 30, 60, 100])\n",
    "ridgecv.fit(X_train, y_train)\n",
    "\n",
    "y_train_rdg = ridgecv.predict(X_train)\n",
    "y_test_rdg = ridgecv.predict(X_test)\n",
    "rdg_prediction = ridgecv.predict(test)\n",
    "\n",
    "#ridgecv_score = ridgecv.score(train, ytrain)\n",
    "ridgecv_alpha = ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgecv_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = linear_model.Ridge(alpha=ridgecv_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso RMSE on Test set : 0.10522098912607274\n",
      "Ridge RMSE on Test set : 0.10753575676011559\n",
      "XGB RMSE on Test set : 0.11379053007977571\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso RMSE on Test set :\", np.sqrt(mean_squared_error(y_test,y_test_las)))\n",
    "print(\"Ridge RMSE on Test set :\", np.sqrt(mean_squared_error(y_test,y_test_rdg)))\n",
    "print(\"XGB RMSE on Test set :\", np.sqrt(mean_squared_error(y_test,y_test_xgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE on Test set : 0.1050434414834762\n"
     ]
    }
   ],
   "source": [
    "averaged_test = (y_test_las+y_test_rdg+y_test_xgb)/3\n",
    "print(\"Average RMSE on Test set :\", np.sqrt(mean_squared_error(y_test,averaged_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (las_prediction+rdg_prediction+xgb_prediction)/3\n",
    "pred_df = pd.DataFrame(np.exp(y_pred), index=test_ID, columns=[\"SalePrice\"])\n",
    "pred_df.to_csv('/Users/holy/dsi/module1/averaged_models.csv', \n",
    "               header=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here it is not working yet\n",
    "## Random forest regressors\n",
    "### Random forest parameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfrcv(n_estimators, min_samples_split, max_features, max_depth):\n",
    "    val = cross_val_score(\n",
    "        RFR(n_estimators=int(n_estimators),\n",
    "            min_samples_split=int(min_samples_split),\n",
    "            max_features=min(max_features, 0.999),\n",
    "            max_depth=int(max_depth),\n",
    "            random_state=2,\n",
    "            criterion='mae',\n",
    "        ),\n",
    "        train.values, y=ytrain, scoring='neg_mean_squared_error', cv=5\n",
    "    ).mean()\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   max_depth |   max_features |   min_samples_split |   n_estimators | \n",
      "    1 | 00m20s | \u001b[35m  -0.02239\u001b[0m | \u001b[32m     6.6050\u001b[0m | \u001b[32m        0.1979\u001b[0m | \u001b[32m             5.1960\u001b[0m | \u001b[32m       51.3081\u001b[0m | \n",
      "    2 | 00m37s | \u001b[35m  -0.02206\u001b[0m | \u001b[32m     6.5539\u001b[0m | \u001b[32m        0.2196\u001b[0m | \u001b[32m             5.8926\u001b[0m | \u001b[32m       88.3707\u001b[0m | \n",
      "    3 | 00m47s | \u001b[35m  -0.01877\u001b[0m | \u001b[32m     9.0808\u001b[0m | \u001b[32m        0.5337\u001b[0m | \u001b[32m             8.5704\u001b[0m | \u001b[32m       43.4988\u001b[0m | \n",
      "    4 | 00m36s |   -0.02446 |      5.3131 |         0.3435 |              8.6749 |        60.5250 | \n",
      "    5 | 00m39s |   -0.01884 |     10.4136 |         0.4562 |              2.8188 |        40.6731 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   max_depth |   max_features |   min_samples_split |   n_estimators | \n",
      "    6 | 00m20s |   -0.02108 |     11.5654 |         0.3246 |             14.4415 |        10.0736 | \n",
      "    7 | 00m16s |   -0.02214 |     11.8260 |         0.1746 |             14.0870 |        10.0571 | \n",
      "    8 | 01m59s |   -0.05604 |      2.1599 |         0.9219 |             14.6826 |        99.9553 | \n",
      "    9 | 00m44s |   -0.01969 |     11.9844 |         0.2360 |             14.9755 |        69.6141 | \n",
      "   10 | 03m28s |   -0.01948 |     11.8942 |         0.9689 |              2.0317 |        93.7899 | \n",
      "   11 | 00m29s |   -0.01940 |     12.0000 |         0.1000 |              2.0000 |        70.1706 | \n",
      "   12 | 00m18s |   -0.05391 |      2.2249 |         0.4461 |              2.1395 |        10.1985 | \n",
      "   13 | 01m23s |   -0.02074 |     12.0000 |         0.9990 |             15.0000 |        33.6188 | \n",
      "   14 | 02m34s |   -0.02006 |     11.6151 |         0.9960 |              2.0996 |        62.8531 | \n",
      "   15 | 00m29s |   -0.02022 |     11.8370 |         0.1132 |             14.6720 |        46.5105 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-1.8346674e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 47, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    }
   ],
   "source": [
    "gp_params = {\"alpha\": 1e-5}\n",
    "rfrBO = BayesianOptimization(\n",
    "        rfrcv,\n",
    "        {'n_estimators': (10, 100),\n",
    "        'min_samples_split': (2, 15),\n",
    "        'max_features': (0.1, 0.999),\n",
    "        'max_depth':(2,12)}#min_samples_leaf\n",
    ")\n",
    "rfrBO.maximize(n_iter=10, **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 43.49883128432349,\n",
       " 'min_samples_split': 8.570423823608582,\n",
       " 'max_features': 0.5337067867029887,\n",
       " 'max_depth': 9.080765381947682}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_params = rfrBO.res[\"max\"][\"max_params\"]\n",
    "rfr_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rfr = RFR(n_estimators = int(round(rfr_params[\"n_estimators\"])),\n",
    "                criterion=\"mae\",\n",
    "               min_samples_split = int(round(rfr_params[\"min_samples_split\"])),\n",
    "               max_features = rfr_params[\"max_features\"],\n",
    "               max_depth = int(round(rfr_params[\"max_depth\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, ytrain, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost score: 0.1126 (0.0060)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso score: 0.1100 (0.0048)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_lasso)\n",
    "print(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge score: 0.1118 (0.0043)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_ridge)\n",
    "print(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest regressor score: 0.1380 (0.0055)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_rfr)\n",
    "print(\"Random forest regressor score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915 +/- 0.00 {'meta-svr__C': 0.1, 'meta-svr__gamma': 0.1}\n",
      "0.912 +/- 0.00 {'meta-svr__C': 0.1, 'meta-svr__gamma': 1.0}\n",
      "0.880 +/- 0.01 {'meta-svr__C': 0.1, 'meta-svr__gamma': 10.0}\n",
      "0.923 +/- 0.00 {'meta-svr__C': 1.0, 'meta-svr__gamma': 0.1}\n",
      "0.916 +/- 0.00 {'meta-svr__C': 1.0, 'meta-svr__gamma': 1.0}\n",
      "0.903 +/- 0.01 {'meta-svr__C': 1.0, 'meta-svr__gamma': 10.0}\n",
      "0.915 +/- 0.00 {'meta-svr__C': 10.0, 'meta-svr__gamma': 0.1}\n",
      "0.912 +/- 0.00 {'meta-svr__C': 10.0, 'meta-svr__gamma': 1.0}\n",
      "0.906 +/- 0.00 {'meta-svr__C': 10.0, 'meta-svr__gamma': 10.0}\n",
      "0.912 +/- 0.00 {'meta-svr__C': 100.0, 'meta-svr__gamma': 0.1}\n",
      "0.910 +/- 0.00 {'meta-svr__C': 100.0, 'meta-svr__gamma': 1.0}\n",
      "0.900 +/- 0.00 {'meta-svr__C': 100.0, 'meta-svr__gamma': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Parameter optimization for svr_rbf\n",
    "svr_rbf = SVR(kernel='rbf')\n",
    "regressors = [model_lasso, model_ridge, model_xgb]\n",
    "stregr = StackingRegressor(regressors=regressors, \n",
    "                           meta_regressor=svr_rbf)\n",
    "\n",
    "params = {'meta-svr__C': [0.1, 1.0, 10.0, 100.0],\n",
    "          'meta-svr__gamma': [0.1, 1.0, 10.0]}\n",
    "\n",
    "grid = GridSearchCV(estimator=stregr, \n",
    "                    param_grid=params, \n",
    "                    cv=5,\n",
    "                    refit=True)\n",
    "grid.fit(train, ytrain)\n",
    "\n",
    "for params, mean_score, scores in grid.grid_scores_:\n",
    "        print(\"%0.3f +/- %0.2f %r\"\n",
    "              % (mean_score, scores.std() / 2.0, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta-svr__C': 1.0, 'meta-svr__gamma': 0.1}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf_opt = SVR(kernel='rbf', gamma=0.1, C=1.)\n",
    "stregr_opt = StackingRegressor(regressors=regressors, \n",
    "                           meta_regressor=svr_rbf_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked regressors: 0.1109 (0.0047)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(stregr_opt)\n",
    "print(\"Stacked regressors: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingRegressor(meta_regressor=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
       "         refit=True,\n",
       "         regressors=[Lasso(alpha=0.00040842386526745213, copy_X=True, fit_intercept=True,\n",
       "   max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "   random_state=None, selection='cyclic', tol=0.0001, warm_start=False), Ridge(alpha=12.915496650148853, copy_X=True, fit_intercept=True,\n",
       "   max_iter=...a=1.8796361500915535, scale_pos_weight=1, seed=None,\n",
       "       silent=1, subsample=0.6833400971272943)],\n",
       "         store_train_meta_features=False, use_features_in_secondary=False,\n",
       "         verbose=0)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stregr_opt.fit(train,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = stregr_opt.predict(test)\n",
    "pred_df = pd.DataFrame(np.exp(y_pred), index=test_ID, columns=[\"SalePrice\"])\n",
    "pred_df.to_csv('/Users/holy/dsi/module1/stacked_regressors_test.csv', \n",
    "               header=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

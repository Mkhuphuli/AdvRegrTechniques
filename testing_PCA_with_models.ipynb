{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average the results from lasso, ridge and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train_clean.csv\")\n",
    "test = pd.read_csv(\"./data/test_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the 'Id' column\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "#Now drop the 'Id' colum since it's unnecessary for the prediction process.\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)\n",
    "\n",
    "ytrain = train[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine data\n",
    "train.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
    "test.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "train.drop(['SalePrice'], axis=1, inplace=True)\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MSSubClass should be string\n",
    "all_data[\"MSSubClass\"] = all_data[\"MSSubClass\"].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all continuous variables\n",
    "all_non_object = all_data.dtypes[all_data.dtypes != \"object\"].index.tolist()\n",
    "# do not consider Year,Month and Qual as continuous\n",
    "year_month = [\"YearBuilt\", \"YearRemodAdd\",\"GarageYrBlt\",\"MoSold\",\"YrSold\",\n",
    "              \"OverallQual\",\"OverallCond\"]\n",
    "# numeric_features\n",
    "numeric_features = list(set(all_non_object)-set(year_month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19 skewed numerical features to Box Cox transform\n"
     ]
    }
   ],
   "source": [
    "# Check the skew of all numerical features\n",
    "skewed_feats = all_data[numeric_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "\n",
    "# check skewness of numerical variables\n",
    "skewness = skewness[abs(skewness.Skew)>0.75]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    #all_data[feat] += 1\n",
    "    all_data[feat] = boxcox1p(all_data[feat], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qual_dict = {\"None\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "all_data[\"ExterQual\"] = all_data[\"ExterQual\"].map(qual_dict).astype(int)\n",
    "all_data[\"ExterCond\"] = all_data[\"ExterCond\"].map(qual_dict).astype(int)\n",
    "all_data[\"BsmtQual\"] = all_data[\"BsmtQual\"].map(qual_dict).astype(int)\n",
    "all_data[\"BsmtCond\"] = all_data[\"BsmtCond\"].map(qual_dict).astype(int)\n",
    "all_data[\"HeatingQC\"] = all_data[\"HeatingQC\"].map(qual_dict).astype(int)\n",
    "all_data[\"KitchenQual\"] = all_data[\"KitchenQual\"].map(qual_dict).astype(int)\n",
    "all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].map(qual_dict).astype(int)\n",
    "all_data[\"GarageQual\"] = all_data[\"GarageQual\"].map(qual_dict).astype(int)\n",
    "all_data[\"GarageCond\"] = all_data[\"GarageCond\"].map(qual_dict).astype(int)\n",
    "\n",
    "all_data[\"BsmtExposure\"] = all_data[\"BsmtExposure\"].map(\n",
    "        {\"None\": 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4}).astype(int)\n",
    "\n",
    "bsmt_fin_dict = {\"None\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n",
    "all_data[\"BsmtFinType1\"] = all_data[\"BsmtFinType1\"].map(bsmt_fin_dict).astype(int)\n",
    "all_data[\"BsmtFinType2\"] = all_data[\"BsmtFinType2\"].map(bsmt_fin_dict).astype(int)\n",
    "\n",
    "all_data[\"Functional\"] = all_data[\"Functional\"].map(\n",
    "        {\"None\": 0, \"Sal\": 1, \"Sev\": 2, \"Maj2\": 3, \"Maj1\": 4, \n",
    "         \"Mod\": 5, \"Min2\": 6, \"Min1\": 7, \"Typ\": 8}).astype(int)\n",
    "\n",
    "all_data[\"GarageFinish\"] = all_data[\"GarageFinish\"].map(\n",
    "        {\"None\": 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3}).astype(int)\n",
    "\n",
    "all_data[\"Fence\"] = all_data[\"Fence\"].map(\n",
    "        {\"None\": 0, \"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4}).astype(int)\n",
    "\n",
    "all_data[\"PoolQC\"] = all_data[\"PoolQC\"].map(qual_dict).astype(int)\n",
    "\n",
    "# Most land slopes are gentle; treat the others as \"not gentle\".\n",
    "all_data[\"LandSlope\"] = (all_data[\"LandSlope\"] == \"Gtl\") * 1\n",
    "# IR2 and IR3 don't appear that often, so just make a distinction\n",
    "# between regular and irregular.\n",
    "all_data[\"LotShape\"] = (all_data[\"LotShape\"] == \"Reg\") * 1\n",
    "# Most properties use standard circuit breakers.\n",
    "all_data[\"Electrical\"] = (all_data[\"Electrical\"] == \"SBrkr\") * 1\n",
    "# Most have a paved drive. Treat dirt/gravel and partial pavement\n",
    "# as \"not paved\".\n",
    "all_data[\"PavedDrive\"] = (all_data[\"PavedDrive\"] == \"Y\") * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # label encoding\n",
    "# for c in all_data.columns:\n",
    "#     if all_data[c].dtype == 'object' or c in year_month:\n",
    "#         lbl = preprocessing.LabelEncoder()\n",
    "#         lbl.fit(list(all_data[c].values)) \n",
    "#         all_data[c] = lbl.transform(list(all_data[c].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2917, 233)\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.get_dummies(all_data)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:] #prediction data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, \n",
    "                                                    ytrain,\n",
    "                                                    train_size=1-test_size, \n",
    "                                                    test_size=test_size, \n",
    "                                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(X_train)+len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Selection\n",
    "\n",
    "### PCA- gave a worse result specially with xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature Scaling before applying PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "test = sc.transform(test)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(0.99)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "test = pca.transform(test)\n",
    "\n",
    "cum_explaind_varaince = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00070330956262161341"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(cum_explaind_varaince)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base models\n",
    "\n",
    "### XGBoost\n",
    "#### XGBoost parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma,\n",
    "                 alpha,lambd):\n",
    "\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['cosample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    params['alpha'] = max(alpha, 0)\n",
    "    params['lambda'] = max(lambd, 0)\n",
    "\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain, num_boost_round=num_rounds, nfold=5,\n",
    "             seed=random_state,\n",
    "             callbacks=[xgb.callback.early_stop(50)])\n",
    "\n",
    "    return -cv_result['test-rmse-mean'].values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading data into d-matrices\n",
    "xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "xgtest = xgb.DMatrix(X_test, label=y_test)\n",
    "xgpred = xgb.DMatrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |     alpha |   colsample_bytree |     gamma |     lambd |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[107]\ttrain-rmse:0.163366+0.00381923\ttest-rmse:0.175219+0.0166345\n",
      "\n",
      "    1 | 00m13s | \u001b[35m  -0.17522\u001b[0m | \u001b[32m   0.3964\u001b[0m | \u001b[32m            0.5150\u001b[0m | \u001b[32m   1.1534\u001b[0m | \u001b[32m   0.0005\u001b[0m | \u001b[32m    32.7550\u001b[0m | \u001b[32m            4.1069\u001b[0m | \u001b[32m     0.6348\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[169]\ttrain-rmse:0.0927082+0.00136231\ttest-rmse:0.149661+0.0179419\n",
      "\n",
      "    2 | 00m24s | \u001b[35m  -0.14966\u001b[0m | \u001b[32m   0.0443\u001b[0m | \u001b[32m            0.2817\u001b[0m | \u001b[32m   0.1951\u001b[0m | \u001b[32m   1.7497\u001b[0m | \u001b[32m    40.7913\u001b[0m | \u001b[32m            5.6323\u001b[0m | \u001b[32m     0.8782\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[279]\ttrain-rmse:0.164025+0.00371862\ttest-rmse:0.17452+0.018377\n",
      "\n",
      "    3 | 00m21s |   -0.17452 |    0.7097 |             0.8620 |    1.1416 |    0.4367 |     19.6103 |             4.6028 |      0.6012 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[478]\ttrain-rmse:0.146231+0.0035352\ttest-rmse:0.161844+0.0198788\n",
      "\n",
      "    4 | 00m27s |   -0.16184 |    1.4362 |             0.2361 |    0.4512 |    1.8029 |     30.5391 |             6.0919 |      0.5636 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[332]\ttrain-rmse:0.178926+0.00368256\ttest-rmse:0.187681+0.0178918\n",
      "\n",
      "    5 | 00m28s |   -0.18768 |    0.7803 |             0.1740 |    1.8812 |    0.0373 |     28.6005 |             8.4504 |      0.6267 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |     alpha |   colsample_bytree |     gamma |     lambd |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[214]\ttrain-rmse:0.0641286+0.00133589\ttest-rmse:0.147327+0.0201605\n",
      "\n",
      "    6 | 00m57s | \u001b[35m  -0.14733\u001b[0m | \u001b[32m   2.0000\u001b[0m | \u001b[32m            1.0000\u001b[0m | \u001b[32m   0.0000\u001b[0m | \u001b[32m   2.0000\u001b[0m | \u001b[32m    50.0000\u001b[0m | \u001b[32m            1.0000\u001b[0m | \u001b[32m     1.0000\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[416]\ttrain-rmse:0.0839646+0.000829577\ttest-rmse:0.146433+0.0187648\n",
      "\n",
      "    7 | 00m48s | \u001b[35m  -0.14643\u001b[0m | \u001b[32m   0.1838\u001b[0m | \u001b[32m            0.1281\u001b[0m | \u001b[32m   0.1235\u001b[0m | \u001b[32m   1.7673\u001b[0m | \u001b[32m     2.5431\u001b[0m | \u001b[32m            1.2391\u001b[0m | \u001b[32m     0.6441\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[120]\ttrain-rmse:0.175515+0.00322547\ttest-rmse:0.185754+0.0190687\n",
      "\n",
      "    8 | 01m13s |   -0.18575 |    0.0291 |             0.1779 |    1.9475 |    1.9329 |     40.9424 |             1.0062 |      0.9877 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[506]\ttrain-rmse:0.0280634+0.000554309\ttest-rmse:0.141475+0.0200079\n",
      "\n",
      "    9 | 00m49s | \u001b[35m  -0.14148\u001b[0m | \u001b[32m   0.0295\u001b[0m | \u001b[32m            0.6752\u001b[0m | \u001b[32m   0.0073\u001b[0m | \u001b[32m   0.1782\u001b[0m | \u001b[32m     2.8108\u001b[0m | \u001b[32m            9.9578\u001b[0m | \u001b[32m     0.5879\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[525]\ttrain-rmse:0.024858+0.000214591\ttest-rmse:0.148201+0.015769\n",
      "\n",
      "   10 | 01m15s |   -0.14820 |    0.1899 |             0.7890 |    0.0066 |    1.9426 |     49.1235 |             9.6335 |      0.5997 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[533]\ttrain-rmse:0.0080314+0.000177375\ttest-rmse:0.150175+0.0174688\n",
      "\n",
      "   11 | 01m24s |   -0.15018 |    0.1810 |             0.1000 |    0.0000 |    1.2046 |     49.2833 |             1.0823 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[438]\ttrain-rmse:0.0859554+0.00160361\ttest-rmse:0.145208+0.0183842\n",
      "\n",
      "   12 | 01m33s |   -0.14521 |    1.9526 |             0.2147 |    0.0219 |    0.3535 |      2.4907 |             5.7889 |      0.9774 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[451]\ttrain-rmse:0.0347896+0.000272506\ttest-rmse:0.143553+0.0174704\n",
      "\n",
      "   13 | 01m50s |   -0.14355 |    0.2069 |             0.6084 |    0.0130 |    1.9934 |      3.5259 |             7.9412 |      0.8535 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[95]\ttrain-rmse:0.007612+0.000771227\ttest-rmse:0.162716+0.0161147\n",
      "\n",
      "   14 | 02m00s |   -0.16272 |    0.0000 |             0.1000 |    0.0000 |    0.0000 |     12.1435 |             6.9627 |      1.0000 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[512]\ttrain-rmse:0.0689788+0.0010075\ttest-rmse:0.142913+0.0189302\n",
      "\n",
      "   15 | 01m25s |   -0.14291 |    1.7479 |             0.9897 |    0.0097 |    1.8157 |      2.8902 |             1.4747 |      0.8618 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1011]\ttrain-rmse:0.0455046+0.000494116\ttest-rmse:0.14193+0.0160321\n",
      "\n",
      "   16 | 01m55s |   -0.14193 |    1.4038 |             0.2454 |    0.0001 |    1.5689 |      2.0517 |             9.0856 |      0.5393 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[473]\ttrain-rmse:0.0665158+0.00103273\ttest-rmse:0.143865+0.0188418\n",
      "\n",
      "   17 | 01m46s |   -0.14387 |    2.0000 |             1.0000 |    0.0000 |    1.9759 |     43.0277 |             5.5031 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[601]\ttrain-rmse:0.026732+0.000387504\ttest-rmse:0.139127+0.0184917\n",
      "\n",
      "   18 | 01m06s | \u001b[35m  -0.13913\u001b[0m | \u001b[32m   0.1225\u001b[0m | \u001b[32m            0.9581\u001b[0m | \u001b[32m   0.0056\u001b[0m | \u001b[32m   1.8707\u001b[0m | \u001b[32m     2.2076\u001b[0m | \u001b[32m            2.4462\u001b[0m | \u001b[32m     0.6226\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[593]\ttrain-rmse:0.0667628+0.000849824\ttest-rmse:0.143311+0.0162079\n",
      "\n",
      "   19 | 01m06s |   -0.14331 |    1.3951 |             0.9208 |    0.0144 |    0.2978 |      2.0291 |             9.3166 |      0.9238 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[368]\ttrain-rmse:0.020792+0.00047363\ttest-rmse:0.148867+0.0145394\n",
      "\n",
      "   20 | 01m25s |   -0.14887 |    0.3592 |             0.9491 |    0.0008 |    1.8336 |     38.8020 |             9.6849 |      0.5289 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[372]\ttrain-rmse:0.0848768+0.000930521\ttest-rmse:0.144+0.0196827\n",
      "\n",
      "   21 | 01m13s |   -0.14400 |    1.4863 |             0.9394 |    0.0322 |    0.7172 |      2.2983 |             6.2440 |      0.5547 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[210]\ttrain-rmse:0.0642312+0.000961045\ttest-rmse:0.146966+0.0204614\n",
      "\n",
      "   22 | 01m36s |   -0.14697 |    2.0000 |             0.1000 |    0.0000 |    0.4307 |     46.0324 |             8.6455 |      1.0000 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[252]\ttrain-rmse:0.037245+0.000224442\ttest-rmse:0.144803+0.0179337\n",
      "\n",
      "   23 | 02m15s |   -0.14480 |    0.0971 |             0.9173 |    0.0239 |    0.1130 |     48.8215 |             6.3914 |      0.8431 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[462]\ttrain-rmse:0.0284434+0.000900106\ttest-rmse:0.140993+0.0180665\n",
      "\n",
      "   24 | 01m16s |   -0.14099 |    0.0000 |             1.0000 |    0.0000 |    0.0000 |      2.0000 |             3.8576 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[358]\ttrain-rmse:0.0655518+0.00110192\ttest-rmse:0.143425+0.0179448\n",
      "\n",
      "   25 | 01m08s |   -0.14342 |    0.2718 |             0.4608 |    0.0432 |    1.4502 |      2.0072 |             7.5397 |      0.9759 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[433]\ttrain-rmse:0.0539554+0.00013231\ttest-rmse:0.146539+0.0172703\n",
      "\n",
      "   26 | 01m45s |   -0.14654 |    0.8206 |             0.9877 |    0.0188 |    0.0769 |     41.5600 |             1.4935 |      0.9749 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mkhuphuli\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -7.48941302e-05]), 'nit': 5, 'funcalls': 55, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[176]\ttrain-rmse:0.0033538+0.000456219\ttest-rmse:0.160227+0.0138716\n",
      "\n",
      "   27 | 01m13s |   -0.16023 |    0.0000 |             1.0000 |    0.0000 |    1.6113 |     24.0992 |             1.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[465]\ttrain-rmse:0.0752818+0.000840445\ttest-rmse:0.144449+0.0189678\n",
      "\n",
      "   28 | 01m18s |   -0.14445 |    1.8663 |             0.7033 |    0.0122 |    1.6091 |     20.5341 |             9.9071 |      0.5794 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[306]\ttrain-rmse:0.0003458+1.82472e-05\ttest-rmse:0.154658+0.0149335\n",
      "\n",
      "   29 | 02m07s |   -0.15466 |    0.0000 |             1.0000 |    0.0000 |    2.0000 |     49.8808 |             3.6765 |      1.0000 | \n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[160]\ttrain-rmse:0.0294724+0.000261358\ttest-rmse:0.146792+0.0192081\n",
      "\n",
      "   30 | 02m00s |   -0.14679 |    0.0136 |             0.7802 |    0.0158 |    1.9455 |     21.9294 |             9.4740 |      0.9037 | \n"
     ]
    }
   ],
   "source": [
    "num_rounds = 3000\n",
    "random_state = 42\n",
    "num_iter = 25\n",
    "init_points = 5\n",
    "params = {\n",
    "        'eta': 0.1,\n",
    "        'silent': 1,\n",
    "        'eval_metric': 'rmse',\n",
    "        'verbose_eval': True,\n",
    "        'seed': random_state\n",
    "    }\n",
    "\n",
    "xgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (1, 10),\n",
    "                                                'colsample_bytree': (0.1, 1),\n",
    "                                                'max_depth': (2, 50), #changed max depth from 12 to 50\n",
    "                                                'subsample': (0.5, 1),\n",
    "                                                'gamma': (0, 2),\n",
    "                                                'alpha': (0, 2),\n",
    "                                                'lambd':(0, 2)\n",
    "                                                })\n",
    "\n",
    "xgbBO.maximize(init_points=init_points, n_iter=num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayesian_params = xgbBO.res[\"max\"][\"max_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bayesian_params = {'alpha': 0.12250868020871097,\n",
    " 'colsample_bytree': 0.96,\n",
    " 'gamma': 0.0055747928592750906,\n",
    " 'lambd': 1.8707339442110735,\n",
    " 'max_depth': 2.2076064679490948,\n",
    " 'min_child_weight': 2.4462408869837908,\n",
    " 'subsample': 0.7}#0.62262942029485835   'colsample_bytree': 0.95811915002042114,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# xgb best params with train/test\n",
    "# {'min_child_weight': 3.7316987233785577,\n",
    "#  'colsample_bytree': 0.18788694151572233,\n",
    "#  'max_depth': 2.58392470353414,\n",
    "#  'subsample': 0.6833400971272943,\n",
    "#  'gamma': 0.027285193517196715,\n",
    "#  'alpha': 0.04647160701094655,\n",
    "#  'lambd': 1.8796361500915535}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'no_estimators': 4000,\n",
    "    'max_depth':int(round(bayesian_params[\"max_depth\"])),\n",
    "    'min_child_weight': bayesian_params[\"min_child_weight\"],\n",
    "    'eta':.1,\n",
    "    'subsample': bayesian_params['subsample'],\n",
    "    'colsample_bytree': bayesian_params['colsample_bytree'],\n",
    "    'gamma':bayesian_params['gamma'],\n",
    "    'alpha':bayesian_params['alpha'],\n",
    "    'lambda':bayesian_params[\"lambd\"],\n",
    "    # Other parameters\n",
    "    'objective':'reg:linear',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_boost_round = 3200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #dont run\n",
    "# cv_results = xgb.cv(\n",
    "#     params,\n",
    "#     xgtrain,\n",
    "#     num_boost_round=num_boost_round,\n",
    "#     seed=42,\n",
    "#     nfold=5,\n",
    "#     metrics={'rmse'},\n",
    "#     early_stopping_rounds=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cv_results['test-rmse-mean'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter ETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %time\n",
    "# # This can take some time…\n",
    "# min_rmse = float(\"Inf\")\n",
    "# best_params = None\n",
    "\n",
    "# for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "#     print(\"CV with eta={}\".format(eta))\n",
    "\n",
    "#     # We update our parameters\n",
    "#     params['eta'] = eta\n",
    "\n",
    "#     # Run and time CV\n",
    "#     cv_results = xgb.cv(\n",
    "#         params,\n",
    "#         xgtrain,\n",
    "#         num_boost_round=num_boost_round,\n",
    "#         seed=42,\n",
    "#         nfold=5,\n",
    "#         metrics=['rmse'],\n",
    "#         early_stopping_rounds=10\n",
    "#     )\n",
    "\n",
    "#     # Update best score\n",
    "#     mean_rmse = cv_results['test-rmse-mean'].min()\n",
    "#     boost_rounds = cv_results['test-rmse-mean'].argmin()\n",
    "#     print(\"\\tRMSE {} for {} rounds\\n\".format(mean_rmse, boost_rounds))\n",
    "#     if mean_rmse < min_rmse:\n",
    "#         min_rmse = mean_rmse\n",
    "#         best_params = eta\n",
    "\n",
    "# print(\"Best params: {}, RMSE: {}\".format(best_params, min_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params['eta'] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.12250868020871097,\n",
       " 'colsample_bytree': 0.96,\n",
       " 'eta': 0.01,\n",
       " 'gamma': 0.005574792859275091,\n",
       " 'lambda': 1.8707339442110735,\n",
       " 'max_depth': 2,\n",
       " 'min_child_weight': 2.446240886983791,\n",
       " 'no_estimators': 4000,\n",
       " 'objective': 'reg:linear',\n",
       " 'subsample': 0.7}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(colsample_bytree=params['colsample_bytree'], \n",
    "                             gamma=params['gamma'],\n",
    "                             learning_rate=params['eta'], max_depth=int(round(params['max_depth'])), \n",
    "                             min_child_weight=params['min_child_weight'], n_estimators=4000,\n",
    "                             reg_alpha=params['alpha'], reg_lambda=params['lambda'],\n",
    "                             subsample=params['subsample'], silent=1,\n",
    "                             random_state =42, nthread = -1)\n",
    "#2200\n",
    "model_xgb.fit(X_train, y_train, early_stopping_rounds=10, \n",
    "             eval_set=[(X_test, y_test)], verbose=False)\n",
    "  \n",
    "y_train_xgb = model_xgb.predict(X_train)\n",
    "y_test_xgb = model_xgb.predict(X_test)\n",
    "xgb_prediction = model_xgb.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(np.exp(xgb_prediction), index=test_ID, columns=[\"SalePrice\"])\n",
    "pred_df.to_csv('./data/xgb_regressor_test_pca.csv', \n",
    "               header=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO model parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mkhuphuli\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lassocv = linear_model.LassoCV(cv=10, random_state=5, alphas=[0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n",
    "                          0.3, 0.6, 1, 3, 6, 10, 30, 60, 100])\n",
    "lassocv.fit(X_train, y_train)\n",
    "\n",
    "y_train_las = lassocv.predict(X_train)\n",
    "y_test_las = lassocv.predict(X_test)\n",
    "las_prediction = lassocv.predict(test)\n",
    "\n",
    "# lassocv_score = lassocv.score(train, ytrain)\n",
    "lassocv_alpha = lassocv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassocv_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_lasso = linear_model.Lasso(alpha=lassocv_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge\n",
    "### Ridge model parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridgecv = linear_model.RidgeCV(cv=5, alphas=[0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n",
    "                          0.3, 0.6, 1, 3, 6, 10, 30, 60, 100])\n",
    "ridgecv.fit(X_train, y_train)\n",
    "\n",
    "y_train_rdg = ridgecv.predict(X_train)\n",
    "y_test_rdg = ridgecv.predict(X_test)\n",
    "rdg_prediction = ridgecv.predict(test)\n",
    "\n",
    "#ridgecv_score = ridgecv.score(train, ytrain)\n",
    "ridgecv_alpha = ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgecv_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ridge = linear_model.Ridge(alpha=ridgecv_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso RMSE on Test set : 0.105220989126\n",
      "Ridge RMSE on Test set : 0.10753575676\n",
      "XGB RMSE on Test set : 0.116164914077\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso RMSE on Test set :\", np.sqrt(mean_squared_error(y_test,y_test_las)))\n",
    "print(\"Ridge RMSE on Test set :\", np.sqrt(mean_squared_error(y_test,y_test_rdg)))\n",
    "print(\"XGB RMSE on Test set :\", np.sqrt(mean_squared_error(y_test,y_test_xgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE on Test set : 0.1050434414834762\n"
     ]
    }
   ],
   "source": [
    "averaged_test = (y_test_las+y_test_rdg+y_test_xgb)/3\n",
    "print(\"Average RMSE on Test set :\", np.sqrt(mean_squared_error(y_test,averaged_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = (las_prediction)#+rdg_prediction+xgb_prediction)/3\n",
    "pred_df = pd.DataFrame(np.exp(y_pred), index=test_ID, columns=[\"SalePrice\"])\n",
    "pred_df.to_csv('./data/lasso_models.csv', \n",
    "               header=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here it is not working yet\n",
    "## Random forest regressors\n",
    "### Random forest parameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rfrcv(n_estimators, min_samples_split, max_features, max_depth):\n",
    "    val = cross_val_score(\n",
    "        RFR(n_estimators=int(n_estimators),\n",
    "            min_samples_split=int(min_samples_split),\n",
    "            max_features=min(max_features, 0.999),\n",
    "            max_depth=int(max_depth),\n",
    "            random_state=2,\n",
    "            criterion='mae',\n",
    "        ),\n",
    "        train.values, y=ytrain, scoring='neg_mean_squared_error', cv=5\n",
    "    ).mean()\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   max_depth |   max_features |   min_samples_split |   n_estimators | \n",
      "    1 | 00m20s | \u001b[35m  -0.02239\u001b[0m | \u001b[32m     6.6050\u001b[0m | \u001b[32m        0.1979\u001b[0m | \u001b[32m             5.1960\u001b[0m | \u001b[32m       51.3081\u001b[0m | \n",
      "    2 | 00m37s | \u001b[35m  -0.02206\u001b[0m | \u001b[32m     6.5539\u001b[0m | \u001b[32m        0.2196\u001b[0m | \u001b[32m             5.8926\u001b[0m | \u001b[32m       88.3707\u001b[0m | \n",
      "    3 | 00m47s | \u001b[35m  -0.01877\u001b[0m | \u001b[32m     9.0808\u001b[0m | \u001b[32m        0.5337\u001b[0m | \u001b[32m             8.5704\u001b[0m | \u001b[32m       43.4988\u001b[0m | \n",
      "    4 | 00m36s |   -0.02446 |      5.3131 |         0.3435 |              8.6749 |        60.5250 | \n",
      "    5 | 00m39s |   -0.01884 |     10.4136 |         0.4562 |              2.8188 |        40.6731 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   max_depth |   max_features |   min_samples_split |   n_estimators | \n",
      "    6 | 00m20s |   -0.02108 |     11.5654 |         0.3246 |             14.4415 |        10.0736 | \n",
      "    7 | 00m16s |   -0.02214 |     11.8260 |         0.1746 |             14.0870 |        10.0571 | \n",
      "    8 | 01m59s |   -0.05604 |      2.1599 |         0.9219 |             14.6826 |        99.9553 | \n",
      "    9 | 00m44s |   -0.01969 |     11.9844 |         0.2360 |             14.9755 |        69.6141 | \n",
      "   10 | 03m28s |   -0.01948 |     11.8942 |         0.9689 |              2.0317 |        93.7899 | \n",
      "   11 | 00m29s |   -0.01940 |     12.0000 |         0.1000 |              2.0000 |        70.1706 | \n",
      "   12 | 00m18s |   -0.05391 |      2.2249 |         0.4461 |              2.1395 |        10.1985 | \n",
      "   13 | 01m23s |   -0.02074 |     12.0000 |         0.9990 |             15.0000 |        33.6188 | \n",
      "   14 | 02m34s |   -0.02006 |     11.6151 |         0.9960 |              2.0996 |        62.8531 | \n",
      "   15 | 00m29s |   -0.02022 |     11.8370 |         0.1132 |             14.6720 |        46.5105 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-1.8346674e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 47, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    }
   ],
   "source": [
    "gp_params = {\"alpha\": 1e-5}\n",
    "rfrBO = BayesianOptimization(\n",
    "        rfrcv,\n",
    "        {'n_estimators': (10, 100),\n",
    "        'min_samples_split': (2, 15),\n",
    "        'max_features': (0.1, 0.999),\n",
    "        'max_depth':(2,12)}#min_samples_leaf\n",
    ")\n",
    "rfrBO.maximize(n_iter=10, **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 43.49883128432349,\n",
       " 'min_samples_split': 8.570423823608582,\n",
       " 'max_features': 0.5337067867029887,\n",
       " 'max_depth': 9.080765381947682}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_params = rfrBO.res[\"max\"][\"max_params\"]\n",
    "rfr_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_rfr = RFR(n_estimators = int(round(rfr_params[\"n_estimators\"])),\n",
    "                criterion=\"mae\",\n",
    "               min_samples_split = int(round(rfr_params[\"min_samples_split\"])),\n",
    "               max_features = rfr_params[\"max_features\"],\n",
    "               max_depth = int(round(rfr_params[\"max_depth\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, ytrain, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost score: 0.1135 (0.0057)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso score: 0.1100 (0.0048)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_lasso)\n",
    "print(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge score: 0.1118 (0.0043)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_ridge)\n",
    "print(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest regressor score: 0.1380 (0.0055)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_rfr)\n",
    "print(\"Random forest regressor score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915 +/- 0.00 {'meta-svr__C': 0.1, 'meta-svr__gamma': 0.1}\n",
      "0.912 +/- 0.00 {'meta-svr__C': 0.1, 'meta-svr__gamma': 1.0}\n",
      "0.880 +/- 0.01 {'meta-svr__C': 0.1, 'meta-svr__gamma': 10.0}\n",
      "0.923 +/- 0.00 {'meta-svr__C': 1.0, 'meta-svr__gamma': 0.1}\n",
      "0.916 +/- 0.00 {'meta-svr__C': 1.0, 'meta-svr__gamma': 1.0}\n",
      "0.903 +/- 0.01 {'meta-svr__C': 1.0, 'meta-svr__gamma': 10.0}\n",
      "0.915 +/- 0.00 {'meta-svr__C': 10.0, 'meta-svr__gamma': 0.1}\n",
      "0.912 +/- 0.00 {'meta-svr__C': 10.0, 'meta-svr__gamma': 1.0}\n",
      "0.906 +/- 0.00 {'meta-svr__C': 10.0, 'meta-svr__gamma': 10.0}\n",
      "0.912 +/- 0.00 {'meta-svr__C': 100.0, 'meta-svr__gamma': 0.1}\n",
      "0.910 +/- 0.00 {'meta-svr__C': 100.0, 'meta-svr__gamma': 1.0}\n",
      "0.900 +/- 0.00 {'meta-svr__C': 100.0, 'meta-svr__gamma': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holy/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Parameter optimization for svr_rbf\n",
    "svr_rbf = SVR(kernel='rbf')\n",
    "regressors = [model_lasso, model_ridge, model_xgb,svr_rbf]\n",
    "stregr = StackingRegressor(regressors=regressors, \n",
    "                           meta_regressor=)\n",
    "\n",
    "params = {'meta-svr__C': [0.1, 1.0, 10.0, 100.0],\n",
    "          'meta-svr__gamma': [0.1, 1.0, 10.0]}\n",
    "\n",
    "grid = GridSearchCV(estimator=stregr, \n",
    "                    param_grid=params, \n",
    "                    cv=5,\n",
    "                    refit=True)\n",
    "grid.fit(train, ytrain)\n",
    "\n",
    "for params, mean_score, scores in grid.grid_scores_:\n",
    "        print(\"%0.3f +/- %0.2f %r\"\n",
    "              % (mean_score, scores.std() / 2.0, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta-svr__C': 1.0, 'meta-svr__gamma': 0.1}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svr_rbf_opt = SVR(kernel='rbf', gamma=0.1, C=1.)\n",
    "stregr_opt = StackingRegressor(regressors=regressors, \n",
    "                           meta_regressor=svr_rbf_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked regressors: 0.1109 (0.0047)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(stregr_opt)\n",
    "print(\"Stacked regressors: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingRegressor(meta_regressor=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
       "         refit=True,\n",
       "         regressors=[Lasso(alpha=0.00040842386526745213, copy_X=True, fit_intercept=True,\n",
       "   max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "   random_state=None, selection='cyclic', tol=0.0001, warm_start=False), Ridge(alpha=12.915496650148853, copy_X=True, fit_intercept=True,\n",
       "   max_iter=...a=1.8796361500915535, scale_pos_weight=1, seed=None,\n",
       "       silent=1, subsample=0.6833400971272943)],\n",
       "         store_train_meta_features=False, use_features_in_secondary=False,\n",
       "         verbose=0)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stregr_opt.fit(train,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = stregr_opt.predict(test)\n",
    "pred_df = pd.DataFrame(np.exp(y_pred), index=test_ID, columns=[\"SalePrice\"])\n",
    "pred_df.to_csv('/Users/holy/dsi/module1/stacked_regressors_test.csv', \n",
    "               header=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "svr = SVR(kernel='linear')\n",
    "lasso = Lasso()\n",
    "rf = RandomForestRegressor(n_estimators=5, \n",
    "                           random_state=RANDOM_SEED)\n",
    "\n",
    "# The StackingCVRegressor uses scikit-learn's check_cv\n",
    "# internally, which doesn't support a random seed. Thus\n",
    "# NumPy's random seed need to be specified explicitely for\n",
    "# deterministic behavior\n",
    "np.random.seed(RANDOM_SEED)\n",
    "stack = StackingCVRegressor(regressors=(svr, lasso, rf),\n",
    "                            meta_regressor=lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': 5,\n",
       " 'lasso': Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "    normalize=False, positive=False, precompute=False, random_state=None,\n",
       "    selection='cyclic', tol=0.0001, warm_start=False),\n",
       " 'lasso__alpha': 1.0,\n",
       " 'lasso__copy_X': True,\n",
       " 'lasso__fit_intercept': True,\n",
       " 'lasso__max_iter': 1000,\n",
       " 'lasso__normalize': False,\n",
       " 'lasso__positive': False,\n",
       " 'lasso__precompute': False,\n",
       " 'lasso__random_state': None,\n",
       " 'lasso__selection': 'cyclic',\n",
       " 'lasso__tol': 0.0001,\n",
       " 'lasso__warm_start': False,\n",
       " 'meta-lasso': Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "    normalize=False, positive=False, precompute=False, random_state=None,\n",
       "    selection='cyclic', tol=0.0001, warm_start=False),\n",
       " 'meta-lasso__alpha': 1.0,\n",
       " 'meta-lasso__copy_X': True,\n",
       " 'meta-lasso__fit_intercept': True,\n",
       " 'meta-lasso__max_iter': 1000,\n",
       " 'meta-lasso__normalize': False,\n",
       " 'meta-lasso__positive': False,\n",
       " 'meta-lasso__precompute': False,\n",
       " 'meta-lasso__random_state': None,\n",
       " 'meta-lasso__selection': 'cyclic',\n",
       " 'meta-lasso__tol': 0.0001,\n",
       " 'meta-lasso__warm_start': False,\n",
       " 'meta_regressor': Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "    normalize=False, positive=False, precompute=False, random_state=None,\n",
       "    selection='cyclic', tol=0.0001, warm_start=False),\n",
       " 'randomforestregressor': RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       " 'randomforestregressor__bootstrap': True,\n",
       " 'randomforestregressor__criterion': 'mse',\n",
       " 'randomforestregressor__max_depth': None,\n",
       " 'randomforestregressor__max_features': 'auto',\n",
       " 'randomforestregressor__max_leaf_nodes': None,\n",
       " 'randomforestregressor__min_impurity_decrease': 0.0,\n",
       " 'randomforestregressor__min_impurity_split': None,\n",
       " 'randomforestregressor__min_samples_leaf': 1,\n",
       " 'randomforestregressor__min_samples_split': 2,\n",
       " 'randomforestregressor__min_weight_fraction_leaf': 0.0,\n",
       " 'randomforestregressor__n_estimators': 5,\n",
       " 'randomforestregressor__n_jobs': 1,\n",
       " 'randomforestregressor__oob_score': False,\n",
       " 'randomforestregressor__random_state': 42,\n",
       " 'randomforestregressor__verbose': 0,\n",
       " 'randomforestregressor__warm_start': False,\n",
       " 'refit': True,\n",
       " 'regressors': (SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "    kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
       "  Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "     normalize=False, positive=False, precompute=False, random_state=None,\n",
       "     selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
       "             oob_score=False, random_state=42, verbose=0, warm_start=False)),\n",
       " 'shuffle': True,\n",
       " 'store_train_meta_features': False,\n",
       " 'svr': SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "   kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
       " 'svr__C': 1.0,\n",
       " 'svr__cache_size': 200,\n",
       " 'svr__coef0': 0.0,\n",
       " 'svr__degree': 3,\n",
       " 'svr__epsilon': 0.1,\n",
       " 'svr__gamma': 'auto',\n",
       " 'svr__kernel': 'linear',\n",
       " 'svr__max_iter': -1,\n",
       " 'svr__shrinking': True,\n",
       " 'svr__tol': 0.001,\n",
       " 'svr__verbose': False,\n",
       " 'use_features_in_secondary': False}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
